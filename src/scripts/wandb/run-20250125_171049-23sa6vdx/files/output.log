Device Map: 1
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.08it/s]
Budget set to: 100
No Test Set Found. Will be using LIMA.
/home/hdiddee/environments/test/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '0.13.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/home/hdiddee/environments/test/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/home/hdiddee/environments/test/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
[2025-01-25 17:10:58,852] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hdiddee/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Traceback (most recent call last):
  File "/home/hdiddee/research/instruction_distillation/ChasingRandom/src/scripts/../train.py", line 218, in <module>
    main()
  File "/home/hdiddee/research/instruction_distillation/ChasingRandom/src/scripts/../train.py", line 205, in main
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/transformers/trainer.py", line 2534, in _inner_training_loop
    self.optimizer.step()
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/accelerate/optimizer.py", line 172, in step
    self.optimizer.step(closure)
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 284, in step
    self.init_state(group, p, gindex, pindex)
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 441, in init_state
    state["state1"] = self.get_state_buffer(p, dtype=torch.float32)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 325, in get_state_buffer
    F.fill(buff, 0)
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/functional.py", line 237, in fill
    elementwise_func("fill", A, None, value)
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/functional.py", line 233, in elementwise_func
    torch.cuda.synchronize()
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/cuda/__init__.py", line 892, in synchronize
    return torch._C._cuda_synchronize()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hdiddee/research/instruction_distillation/ChasingRandom/src/scripts/../train.py", line 218, in <module>
[rank1]:     main()
[rank1]:   File "/home/hdiddee/research/instruction_distillation/ChasingRandom/src/scripts/../train.py", line 205, in main
[rank1]:     train_result = trainer.train()
[rank1]:                    ^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/transformers/trainer.py", line 2123, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/transformers/trainer.py", line 2534, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/accelerate/optimizer.py", line 172, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 284, in step
[rank1]:     self.init_state(group, p, gindex, pindex)
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 441, in init_state
[rank1]:     state["state1"] = self.get_state_buffer(p, dtype=torch.float32)
[rank1]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 325, in get_state_buffer
[rank1]:     F.fill(buff, 0)
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/functional.py", line 237, in fill
[rank1]:     elementwise_func("fill", A, None, value)
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/functional.py", line 233, in elementwise_func
[rank1]:     torch.cuda.synchronize()
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/cuda/__init__.py", line 892, in synchronize
[rank1]:     return torch._C._cuda_synchronize()
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: KeyboardInterrupt
