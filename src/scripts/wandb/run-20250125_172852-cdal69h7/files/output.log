Device Map: 1
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.06s/it]
Budget set to: 100
No Test Set Found. Will be using LIMA.
/home/hdiddee/environments/test/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '0.13.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/home/hdiddee/environments/test/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/home/hdiddee/environments/test/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
[2025-01-25 17:29:00,144] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/hdiddee/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Traceback (most recent call last):
  File "/home/hdiddee/research/instruction_distillation/ChasingRandom/src/scripts/../train.py", line 218, in <module>
    main()
  File "/home/hdiddee/research/instruction_distillation/ChasingRandom/src/scripts/../train.py", line 205, in main
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/transformers/trainer.py", line 2534, in _inner_training_loop
    self.optimizer.step()
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/accelerate/optimizer.py", line 172, in step
    self.optimizer.step(closure)
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 286, in step
    self.prefetch_state(p)
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 335, in prefetch_state
    F.prefetch_tensor(state["state1"])
  File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/functional.py", line 206, in prefetch_tensor
    lib.cprefetch(get_ptr(A), ct.c_size_t(num_bytes), ct.c_int32(deviceid))
KeyboardInterrupt
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/hdiddee/research/instruction_distillation/ChasingRandom/src/scripts/../train.py", line 218, in <module>
[rank1]:     main()
[rank1]:   File "/home/hdiddee/research/instruction_distillation/ChasingRandom/src/scripts/../train.py", line 205, in main
[rank1]:     train_result = trainer.train()
[rank1]:                    ^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/transformers/trainer.py", line 2123, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/transformers/trainer.py", line 2534, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/accelerate/optimizer.py", line 172, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 286, in step
[rank1]:     self.prefetch_state(p)
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/optim/optimizer.py", line 335, in prefetch_state
[rank1]:     F.prefetch_tensor(state["state1"])
[rank1]:   File "/home/hdiddee/environments/test/lib/python3.12/site-packages/bitsandbytes/functional.py", line 206, in prefetch_tensor
[rank1]:     lib.cprefetch(get_ptr(A), ct.c_size_t(num_bytes), ct.c_int32(deviceid))
[rank1]: KeyboardInterrupt
